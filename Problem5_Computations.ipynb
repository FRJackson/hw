{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FRJackson/hw/blob/main/Problem5_Computations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbhGeE4GtCya"
      },
      "source": [
        "# Problem 5: Model parameter computations\n",
        "\n",
        "In this problem we will refer to some of the architecture the previous problems were based on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pr7145wuK6o"
      },
      "source": [
        "## MNIST and synthetic example's MLPs\n",
        "\n",
        "(1) Derive **by hand** the number of parameters from the architectures used in the MNIST and synthetic example's MLPs.\n",
        "\n",
        "  * MNIST MLP is a feedforward neural network with one hidden layer of width 20 for MNIST classification. Use ReLU in the hidden layer and an output layer of size 10. Note that the images are 28 by 28 grayscale.\n",
        "  * Synthetic example's MLP is given as below\n",
        "  ```{python}\n",
        "    # Synthetic example's MLP\n",
        "    class SmallMLP(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(SmallMLP, self).__init__()\n",
        "            self.layers = nn.Sequential(\n",
        "                nn.Linear(1, 8),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(8, 8),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(8, 1)\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.layers(x)\n",
        "  ```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuxuujD9vn5v"
      },
      "source": [
        "**Your answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14UV3myxvryF"
      },
      "source": [
        "(2) Check your answer by using `model.parameters` to compute the total number of parameters of both models.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0YoyTmfv_Q4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7560a1c-7661-4fc9-daa8-fd54ff2f38d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential total params = 15910\n",
            "  0.weight                       shape=(20, 784) numel=15680\n",
            "  0.bias                         shape=(20,) numel=20\n",
            "  2.weight                       shape=(10, 20) numel=200\n",
            "  2.bias                         shape=(10,) numel=10\n",
            "SmallMLP total params = 97\n",
            "  layers.0.weight                shape=(8, 1) numel=8\n",
            "  layers.0.bias                  shape=(8,) numel=8\n",
            "  layers.2.weight                shape=(8, 8) numel=64\n",
            "  layers.2.bias                  shape=(8,) numel=8\n",
            "  layers.4.weight                shape=(1, 8) numel=8\n",
            "  layers.4.bias                  shape=(1,) numel=1\n",
            "same\n"
          ]
        }
      ],
      "source": [
        "##### YOUR CODE HERE #####\n",
        "# 导入 PyTorch 的神经网络模块\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 定义 MNIST 用的一层隐藏层 MLP：784 -> 20 -> 10\n",
        "mnist_mlp = nn.Sequential(                 # nn.Sequential 顺序地把若干层串起来\n",
        "    nn.Linear(784, 20),                    # 线性层：输入维度 784（28*28 像素），输出维度 20（隐藏宽度）\n",
        "    nn.ReLU(),                             # ReLU 激活层（不含可训练参数）\n",
        "    nn.Linear(20, 10)                      # 线性层：输入 20，输出 10（10 个类别）\n",
        ")\n",
        "\n",
        "# 按题面给出的合成样例 SmallMLP：1 -> 8 -> 8 -> 1\n",
        "class SmallMLP(nn.Module):                  # 自定义一个 nn.Module 子类\n",
        "    def __init__(self):                     # 构造函数：定义网络结构\n",
        "        super(SmallMLP, self).__init__()    # 调用父类构造，完成基础初始化\n",
        "        self.layers = nn.Sequential(        # 把若干层打包为一个顺序容器\n",
        "            nn.Linear(1, 8),                # 线性层：输入 1，输出 8\n",
        "            nn.ReLU(),                      # ReLU 激活（无参数）\n",
        "            nn.Linear(8, 8),                # 线性层：输入 8，输出 8\n",
        "            nn.ReLU(),                      # ReLU 激活（无参数）\n",
        "            nn.Linear(8, 1)                 # 线性层：输入 8，输出 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):                    # 前向传播：定义数据如何流过各层\n",
        "        return self.layers(x)                # 依次通过顺序容器中的各层并返回结果\n",
        "\n",
        "small_mlp = SmallMLP()                       # 实例化合成样例模型\n",
        "\n",
        "# 一个通用的小工具函数：统计并打印模型的总参数量与逐层形状\n",
        "def count_params(model):                     # 定义函数：输入是任意 nn.Module\n",
        "    total = sum(p.numel() for p in model.parameters())  # numel() 统计张量中元素总数；对所有参数求和\n",
        "    print(model.__class__.__name__, 'total params =', total)  # 打印模型名和总参数量\n",
        "    for name, p in model.named_parameters():             # 遍历具名参数，便于核对每一层的形状\n",
        "        print(f'  {name:30s} shape={tuple(p.shape)} numel={p.numel()}')\n",
        "    return total                                         # 返回总参数量，便于需要时进一步使用\n",
        "\n",
        "# 分别统计两个模型的参数量（应得到：MNIST MLP=15910，SmallMLP=97）\n",
        "mnist_total = count_params(mnist_mlp)         # 统计 MNIST MLP 的参数\n",
        "small_total = count_params(small_mlp)         # 统计 SmallMLP 的参数\n",
        "\n",
        "# 也可以做一个简单的断言，和手算结果核对，若不一致会抛出 AssertionError\n",
        "assert mnist_total == 15910, f'期望 15910，得到 {mnist_total}'\n",
        "assert small_total == 97,    f'期望 97，得到 {small_total}'\n",
        "print('same')               # 若通过断言，打印确认信息\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMSRFsyj2lcq"
      },
      "source": [
        "## CNN and MobileNetV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-pxTgyw2oKn"
      },
      "source": [
        "(3) We are now refering to the models used in Problem 4. The CNN given in the problem is replicated.\n",
        "```{python}\n",
        "\n",
        "original_CNN = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, kernel_size=3, padding=1),  # 32 filters, 3x3 kernel, input_shape=IMG_SHAPE\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(32, 64, kernel_size=3, padding=1),  # 64 filters, 3x3 kernel\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(64, 64, kernel_size=3, padding=1),  # 64 filters, 3x3 kernel\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * (IMG_SHAPE[0] // 4) * (IMAGE_SHAPE[0] // 4), 128),  # Adjust input size based on image shape\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(128, 102),  # Adjust output size based on the number of classes (102),\n",
        ")\n",
        "```\n",
        " * Change the number of filters to half of the original amount in each Conv2d layer as well as the size of images (224x224 to 8x8) and assume there is only 50 classes. Assume that the image is still RGB with 3 channels.\n",
        " * Compute the number of parameters **by hand** of the first Conv layer, ReLU and MaxPool layer. Make sure to break down your calculations in terms of the number of filters, weights per filters (note that default layers include `bias=True`). What does ReLU and MaxPool do in terms of model parameter counts?\n",
        " * Use `model.parameters` to access the total number of parameters of the CNN and the number of parameters of the first block of Conv2d+ReLU+MaxPool2d."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uReAeKku4uan"
      },
      "source": [
        "**Your answer**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b16Mzj8X4_HF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa01767e-f788-4cb5-bc5f-8d0d974a77d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Q3: Modified CNN Stats ===\n",
            "Hand-count first block params (Conv2d only; ReLU/Pool have 0): 448\n",
            "Code-count first block params (Conv2d):                          448\n",
            "Total params of modified CNN:                                    37298\n",
            "both 448）\n"
          ]
        }
      ],
      "source": [
        "##### YOUR CODE HERE #####\n",
        "# ====== Modified CNN for Q3 ======\n",
        "\n",
        "import torch                           # 引入 PyTorch 主包（张量与自动求导）\n",
        "import torch.nn as nn                  # 引入神经网络模块（层、激活、容器等）\n",
        "\n",
        "# 题面要求：把每个卷积层的 filter 数量减半，且输入图片从 224x224 改为 8x8，并且类别数为 50。\n",
        "# 原网络：Conv2d(3->32) -> ReLU -> MaxPool2d -> Conv2d(32->64) -> ReLU -> MaxPool2d\n",
        "#       -> Conv2d(64->64) -> ReLU -> Flatten -> Linear(64 * (H//4) * (W//4) -> 128) -> ReLU -> Dropout -> Linear(128 -> 102)\n",
        "# 修改后：Conv2d(3->16) -> ReLU -> MaxPool2d -> Conv2d(16->32) -> ReLU -> MaxPool2d\n",
        "#       -> Conv2d(32->32) -> ReLU -> Flatten -> Linear(32 * (8//4) * (8//4) -> 128) -> ReLU -> Dropout -> Linear(128 -> 50)\n",
        "\n",
        "# 为了便于统计分块参数，使用具名 Sequential（便于按名字取到第一块）\n",
        "from collections import OrderedDict    # 用于给 nn.Sequential 指定每一层的名字，便于后续统计\n",
        "\n",
        "IMG_H = 8                              # 输入图像高度 8（题面要求）\n",
        "IMG_W = 8                              # 输入图像宽度 8（题面要求）\n",
        "NUM_CLASSES = 50                       # 类别数 50（题面要求）\n",
        "\n",
        "# 第一、二次池化（stride=2）后，空间尺寸分别变为：8 -> 4 -> 2\n",
        "# 因此 Flatten 前的空间尺寸应为 2x2，最后一层卷积的输出通道数为 32\n",
        "flatten_in_features = 32 * (IMG_H // 4) * (IMG_W // 4)   # = 32 * 2 * 2 = 128\n",
        "\n",
        "# 构建修改后的 CNN（与题面结构一致，仅改变通道数与最终分类头）\n",
        "modified_cnn = nn.Sequential(OrderedDict([\n",
        "    ('conv1',   nn.Conv2d(in_channels=3,  out_channels=16, kernel_size=3, padding=1, bias=True)),  # 第一层卷积：3->16, 3x3, padding=1\n",
        "    ('relu1',   nn.ReLU()),                                                                        # 第一层 ReLU（无可训练参数）\n",
        "    ('pool1',   nn.MaxPool2d(kernel_size=2, stride=2)),                                            # 第一层 2x2 最大池化（无可训练参数）\n",
        "    ('conv2',   nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, bias=True)),  # 第二层卷积：16->32\n",
        "    ('relu2',   nn.ReLU()),                                                                        # 第二层 ReLU（无可训练参数）\n",
        "    ('pool2',   nn.MaxPool2d(kernel_size=2, stride=2)),                                            # 第二层 2x2 最大池化（无可训练参数）\n",
        "    ('conv3',   nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1, bias=True)),  # 第三层卷积：32->32（保持通道）\n",
        "    ('relu3',   nn.ReLU()),                                                                        # 第三层 ReLU（无可训练参数）\n",
        "    ('flatten', nn.Flatten()),                                                                      # 展平为一维向量\n",
        "    ('fc1',     nn.Linear(in_features=flatten_in_features, out_features=128, bias=True)),           # 全连接：128 -> 128（因上面计算得到 32*2*2=128）\n",
        "    ('relu4',   nn.ReLU()),                                                                         # 全连接后的 ReLU\n",
        "    ('drop',    nn.Dropout(p=0.2)),                                                                 # Dropout（无可训练参数）\n",
        "    ('fc2',     nn.Linear(in_features=128, out_features=NUM_CLASSES, bias=True)),                   # 分类头：128 -> 50（题面要求）\n",
        "]))\n",
        "\n",
        "# -------- (A) 手算第一块（Conv2d+ReLU+MaxPool2d）的参数数量 --------\n",
        "# 仅 Conv2d(3 -> 16, k=3x3, bias=True) 有可训练参数：\n",
        "#   权重参数量 = out_channels * in_channels * kH * kW = 16 * 3 * 3 * 3 = 432\n",
        "#   偏置参数量 = out_channels = 16\n",
        "#   第一块合计 = 432 + 16 = 448\n",
        "hand_count_first_block = 16 * 3 * 3 * 3 + 16\n",
        "\n",
        "# ReLU 与 MaxPool2d 作用说明（参数角度）：\n",
        "# - ReLU：逐元素非线性激活，**不含可训练参数**（参数量 = 0）。\n",
        "# - MaxPool2d：下采样操作（取局部最大值），**不含可训练参数**（参数量 = 0）。\n",
        "\n",
        "# -------- (B) 用 model.parameters() 统计整个模型 & 第一块参数数量 --------\n",
        "def count_params(module: nn.Module) -> int:\n",
        "    \"\"\"统计任意模块的参数总数（元素个数之和）\"\"\"\n",
        "    return sum(p.numel() for p in module.parameters())\n",
        "\n",
        "total_params = count_params(modified_cnn)         # 整个 CNN 的参数总数\n",
        "first_block_params = count_params(modified_cnn.conv1)  # 第一块（Conv2d）参数总数；ReLU/MaxPool 无参数\n",
        "\n",
        "# -------- (C) 打印核对结果 --------\n",
        "print(\"=== Q3: Modified CNN Stats ===\")\n",
        "print(\"Hand-count first block params (Conv2d only; ReLU/Pool have 0):\", hand_count_first_block)\n",
        "print(\"Code-count first block params (Conv2d):                         \", first_block_params)\n",
        "print(\"Total params of modified CNN:                                   \", total_params)\n",
        "\n",
        "# 断言核对手算与代码是否一致（若不一致会抛错，方便作业自检）\n",
        "assert hand_count_first_block == first_block_params, \"第一块参数量：手算与代码统计不一致！\"\n",
        "print(\"both 448）\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIE6PKlPvSHd"
      },
      "source": [
        "We now load the pretrained MobileNetV2 architecture. We remove the classifier head as in problem 4 and we will be counting the number of parameters in the blocks of the backbone architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLRCJuqK5FuS"
      },
      "source": [
        "(4) Can you compute **by hand** how many parameters are in an inverted residual block? For each feature input in a BatchNorm layer, the number of learnable parameters is doubled. Assume the following structure for an inverted residual block:\n",
        "```{python}\n",
        "(conv): Sequential(\n",
        "(0): Conv2dNormActivation(\n",
        "       (0): Conv2d(C_in, C_mid, kernel_size=k1, stride=s1, padding=p1, bias=False)\n",
        "       (1): BatchNorm2d(C_mid)\n",
        "       (2): ReLU6\n",
        "     )\n",
        "(1): Conv2dNormActivation(\n",
        "       (0): Conv2d(C_mid, C_mid, kernel_size=k2, stride=s2, padding=p2, group=C_mid, bias=False)\n",
        "       (1): BatchNorm2d(C_mid)\n",
        "       (2): ReLU6\n",
        "     )\n",
        "(2): Conv2d(C_mid, C_out, kernel_size=k3, stride=s3, padding=p3, bias=False)\n",
        "(3): BatchNorm2d(C_out)\n",
        "\n",
        "```\n",
        "Your answer should be a function of $C_{in}, C_{mid}, C_{out}, k_1, s_1, p_1, k_2, s_2, p_2, k_3, s_3, p_3$. Double-check your expression by plugging in the values for the 2nd inverted layer and compare to what you get using `model.parameters`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePP2ITpcAYyC"
      },
      "source": [
        "**Your answer**:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# —— 选用一组常见的 MobileNetV2 第2个 inverted block 的参数做演示 ——\n",
        "C_in, C_mid, C_out = 16, 96, 24   # 典型：t=6 扩张，16→96→24\n",
        "k1, k2, k3 = 1, 3, 1              # 1x1 expand, 3x3 depthwise, 1x1 project\n",
        "s1, s2, s3 = 1, 2, 1              # 步幅仅影响特征图尺寸，不影响参数量\n",
        "p1, p2, p3 = 0, 1, 0              # 填充同理\n",
        "\n",
        "# —— 按题面结构搭建该 block（bias=False；BN 有可学习的 gamma/beta）——\n",
        "block = nn.Sequential(\n",
        "    nn.Sequential(                                # ① expand: C_in -> C_mid\n",
        "        nn.Conv2d(C_in, C_mid, k1, s1, p1, bias=False),\n",
        "        nn.BatchNorm2d(C_mid),\n",
        "        nn.ReLU6(inplace=False),\n",
        "    ),\n",
        "    nn.Sequential(                                # ② depthwise: C_mid -> C_mid（groups=C_mid）\n",
        "        nn.Conv2d(C_mid, C_mid, k2, s2, p2, groups=C_mid, bias=False),\n",
        "        nn.BatchNorm2d(C_mid),\n",
        "        nn.ReLU6(inplace=False),\n",
        "    ),\n",
        "    nn.Conv2d(C_mid, C_out, k3, s3, p3, bias=False),  # ③ project: C_mid -> C_out\n",
        "    nn.BatchNorm2d(C_out),\n",
        ")\n",
        "\n",
        "# —— 代码统计参数量 ——\n",
        "def count_params(m: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in m.parameters())\n",
        "\n",
        "code_total = count_params(block)\n",
        "\n",
        "# —— 手算带入上述数值 ——\n",
        "hand_total = (\n",
        "    C_mid*C_in*(k1**2)      # expand conv\n",
        "  + C_mid*(k2**2)           # depthwise conv\n",
        "  + C_out*C_mid*(k3**2)     # project conv\n",
        "  + 4*C_mid + 2*C_out       # BN(C_mid)+BN(C_mid)+BN(C_out)\n",
        ")\n",
        "\n",
        "print(\"Hand-count total:\", hand_total)  # 期望 5136\n",
        "print(\"Code-count total:\", code_total)\n",
        "\n",
        "assert hand_total == code_total, \"手算与代码统计不一致！\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuksDHCmDrBk",
        "outputId": "d37b3ec1-e0d3-46e6-8623-3e3e65fc16c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hand-count total: 5136\n",
            "Code-count total: 5136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0brrBt5AdTG"
      },
      "source": [
        "(5) The full MobileNetV2 after removing classifier head has 19 blocks.\n",
        "  * Can you use the same code as before to plot the number of parameters in each block?\n",
        "  * In Problem 4 you have frozen all parameters but the last three blocks. How many trainable parameters do you get on the backbone after unfreezing the top three blocks? You can reuse code given in Problem 4.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmaqlRMAtDBP",
        "outputId": "db032d60-5d72-4ee7-b579-668dc4abf906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 85.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MobileNetV2(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (16): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (17): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (18): Conv2dNormActivation(\n",
              "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Identity()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "# MobileNetV2 model\n",
        "MobileNetV2 = torchvision.models.mobilenet_v2(pretrained=True)\n",
        "MobileNetV2.classifier = nn.Identity()  # Remove the classifier layers\n",
        "MobileNetV2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4Ppmd5gBOrP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36393a4b-e44f-42b1-dec1-34a4eae612ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== (1) Number of parameters in each block ===\n",
            "Block 01:        928 parameters\n",
            "Block 02:        896 parameters\n",
            "Block 03:       5136 parameters\n",
            "Block 04:       8832 parameters\n",
            "Block 05:      10000 parameters\n",
            "Block 06:      14848 parameters\n",
            "Block 07:      14848 parameters\n",
            "Block 08:      21056 parameters\n",
            "Block 09:      54272 parameters\n",
            "Block 10:      54272 parameters\n",
            "Block 11:      54272 parameters\n",
            "Block 12:      66624 parameters\n",
            "Block 13:     118272 parameters\n",
            "Block 14:     118272 parameters\n",
            "Block 15:     155264 parameters\n",
            "Block 16:     320000 parameters\n",
            "Block 17:     320000 parameters\n",
            "Block 18:     473920 parameters\n",
            "Block 19:     412160 parameters\n",
            "\n",
            "Total parameters (backbone only): 2,223,872\n"
          ]
        }
      ],
      "source": [
        "##### YOUR CODE HERE #####\n",
        "# Number of parameters in each block\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "# ====== 加载预训练的 MobileNetV2 模型并去掉分类头 ======\n",
        "MobileNetV2 = torchvision.models.mobilenet_v2(pretrained=True)\n",
        "MobileNetV2.classifier = nn.Identity()   # 移除分类器，仅保留 19 个 backbone blocks\n",
        "\n",
        "# ====== 统计每个 block 的参数数量 ======\n",
        "print(\"=== (1) Number of parameters in each block ===\")\n",
        "\n",
        "# MobileNetV2.features 是包含 19 个 inverted residual blocks 的顺序容器\n",
        "for i, block in enumerate(MobileNetV2.features):\n",
        "    # 每个 block 的参数数量 = 各参数张量元素总数之和\n",
        "    num_params = sum(p.numel() for p in block.parameters())\n",
        "    print(f\"Block {i+1:02d}: {num_params:>10} parameters\")\n",
        "\n",
        "# 统计整个主干（backbone）的总参数数量（不包括分类层）\n",
        "total_params = sum(p.numel() for p in MobileNetV2.features.parameters())\n",
        "print(f\"\\nTotal parameters (backbone only): {total_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkxIwuVRCPjP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c04da5-6f3a-42f1-f0b4-45e7fc3294df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== (2) Trainable Parameters Summary ===\n",
            "Trainable parameters (only top 3 blocks unfrozen): 1,206,080\n",
            "Frozen parameters: 1,017,792\n",
            "Total backbone parameters check: 2,223,872\n"
          ]
        }
      ],
      "source": [
        "##### YOUR CODE HERE #####\n",
        "# Number of trainable parameters after only unfreezing top 3 blocks\n",
        "# ====== 冻结所有参数（不参与训练） ======\n",
        "for param in MobileNetV2.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# ====== 解冻最后三个 block（使其重新可训练） ======\n",
        "for block in MobileNetV2.features[-3:]:\n",
        "    for param in block.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# ====== 统计可训练与冻结的参数数量 ======\n",
        "trainable_params = sum(p.numel() for p in MobileNetV2.features.parameters() if p.requires_grad)\n",
        "frozen_params = sum(p.numel() for p in MobileNetV2.features.parameters() if not p.requires_grad)\n",
        "\n",
        "print(\"=== (2) Trainable Parameters Summary ===\")\n",
        "print(f\"Trainable parameters (only top 3 blocks unfrozen): {trainable_params:,}\")\n",
        "print(f\"Frozen parameters: {frozen_params:,}\")\n",
        "print(f\"Total backbone parameters check: {trainable_params + frozen_params:,}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}